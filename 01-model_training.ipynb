{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is developed using ml.t3.medium instance with `Python 3 (Data Science)` kernel on SageMaker Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import SageMaker SDK and Create a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "aws_region = session.boto_region_name\n",
    "\n",
    "# Project Bucket\n",
    "bucket = session.default_bucket()\n",
    "dataset_prefix = 'medical-imaging-workshop/dataset'\n",
    "scaled_dataset_prefix = 'medical-imaging-workshop/scaled_dataset'\n",
    "scaled_zipped_dataset_prefix = 'medical-imaging-workshop/scaled_zipped_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a SageMaker `PyTorch Estimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "def get_pytorch_estimator(entry_point, hyperparameters, instance_type, \n",
    "                          instance_count, output_prefix, \n",
    "                          dist_training_config=None, volume_size=10, \n",
    "                          subnets=None, security_group_ids=None):\n",
    "    pt_estimator = PyTorch(\n",
    "        role=role,\n",
    "        sagemaker_session=session,\n",
    "        subnets=subnets,\n",
    "        security_group_ids=security_group_ids,\n",
    "\n",
    "        source_dir='src',\n",
    "        entry_point=entry_point,\n",
    "        hyperparameters=hyperparameters,\n",
    "        py_version='py36',\n",
    "        framework_version='1.6.0',\n",
    "\n",
    "        instance_count=instance_count,\n",
    "        instance_type=instance_type,\n",
    "        volume_size=volume_size,\n",
    "\n",
    "        enable_sagemaker_metrics=True,\n",
    "        metric_definitions=metric_def,\n",
    "\n",
    "        debugger_hook_config=False,\n",
    "        disable_profiler=True,\n",
    "        distribution=dist_training_config,\n",
    "\n",
    "        code_location=f's3://{bucket}/{output_prefix}/output',\n",
    "        output_path=f's3://{bucket}/{output_prefix}/output',\n",
    "        max_run=432000 # Max runtime of of 5 days\n",
    "    )\n",
    "    \n",
    "    return pt_estimator\n",
    "\n",
    "# Training loop metrics to persist\n",
    "metric_def = [\n",
    "    {\n",
    "        \"Name\": \"train_loss\",\n",
    "        \"Regex\": \"train_loss: (.*?)$\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"average_loss\",\n",
    "        \"Regex\": \"average loss: (.*?)$\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"mean_dice\",\n",
    "        \"Regex\": \"current mean dice: (.*?) \",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"time_per_epoch\",\n",
    "        \"Regex\": \"secs_time_per_epoch: (.*?)$\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"dice_tc\",\n",
    "        \"Regex\": \"tc: (.*?) \",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"dice_wt\",\n",
    "        \"Regex\": \"wt: (.*?) \",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"dice_et\",\n",
    "        \"Regex\": \"et: (.*?)$\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU Device Experiments - Original Dataset 484 training pairs (4.65 GB)\n",
    "\n",
    "Run training for three of MONAI's dataset classes:\n",
    "1. `Dataset`: standard data loading\n",
    "2. `PersistentDataset`: persist processed data on disk\n",
    "2. `CacheDataset`: persist processed data in CPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data_on_s3 = \"s3://{}/{}/Task01_BrainTumour\".format(bucket, dataset_prefix)\n",
    "\n",
    "hyperparameters = {\n",
    "    'torch_dataset_type': \"Dataset\",\n",
    "    'lr': 5e-3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 4\n",
    "}\n",
    "    \n",
    "    \n",
    "for dataset_type in ['Dataset', 'PersistentDataset', 'CacheDataset']:\n",
    "\n",
    "    hyperparameters[\"torch_dataset_type\"] = dataset_type\n",
    "    \n",
    "    # Instanciate a training container with pytorch image\n",
    "    WORKFLOW_DATE_TIME = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "    output_prefix = \"brats_ebs/{}/{}/sagemaker\".format(WORKFLOW_DATE_TIME, dataset_type)\n",
    "    pt_estimator = get_pytorch_estimator('single_gpu_training.py', \n",
    "                                         hyperparameters, \n",
    "                                         'ml.g5.2xlarge', \n",
    "                                         1, \n",
    "                                         output_prefix, \n",
    "                                         dist_training_config=None, \n",
    "                                         volume_size=100)\n",
    "\n",
    "\n",
    "    # Luanch training job\n",
    "    pt_estimator.fit(\n",
    "        job_name='monai-1gpu-{}-{}'.format(dataset_type, WORKFLOW_DATE_TIME),\n",
    "        inputs={'train':training_data_on_s3},\n",
    "        wait=False\n",
    "    )\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "The above runs should produce 3 training jobs. Visit the SageMaker training jobs for details on each. The `CacheDataset` run should be the fastest, followed by `PersistentDataset` and `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi GPU Device Experiment using SageMaker distributed training library (data parallel)\n",
    "#### Using S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output path: model artifacts and source code\n",
    "TRAINING_JOB_DATETIME = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "output_prefix = \"brats_ebs/{}/sagemaker\".format(TRAINING_JOB_DATETIME)\n",
    "\n",
    "# compute resources\n",
    "instance_type = 'ml.p3.16xlarge'\n",
    "instance_count = 1\n",
    "world_size = instance_count * 8\n",
    "num_vcpu = 64\n",
    "num_workers = 16 \n",
    "\n",
    "# network hyperparameters\n",
    "hyperparameters = {'lr': 1e-4 * world_size,\n",
    "                   'batch_size': 4 * world_size,\n",
    "                   'epochs': 10,\n",
    "                   'num_workers': num_workers\n",
    "                  }\n",
    "\n",
    "dist_config = {'smdistributed':\n",
    "               {'dataparallel':{'enabled': True}}\n",
    "              }\n",
    "\n",
    "pt_estimator = get_pytorch_estimator('multi_gpu_training.py',\n",
    "                                     hyperparameters,\n",
    "                                     instance_type,\n",
    "                                     instance_count,\n",
    "                                     output_prefix,\n",
    "                                     dist_training_config=dist_config,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_estimator.fit(\n",
    "    job_name='brats-{}p316-s3-{}batch-{}worker-{}'.format(instance_count,\n",
    "                                                          4*world_size,\n",
    "                                                          num_workers,\n",
    "                                                          TRAINING_JOB_DATETIME),\n",
    "    inputs={'train':training_data_on_s3},\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "The above run should produce 1 SageMaker training job which uses 8 GPUs in an `ml.p3.16xlarge` instance. Visit the SageMaker training jobs for details."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
